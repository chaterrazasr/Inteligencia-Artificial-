{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOAL7VNC3xkybdPDceiRVcZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PRIMER PARCIAL 02/2024"],"metadata":{"id":"VfQa6YuWzOCm"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0Vd2QmLy9KW","executionInfo":{"status":"ok","timestamp":1726741342765,"user_tz":240,"elapsed":29565,"user":{"displayName":"Charly Alexandre Terrazas Rosado","userId":"18005393683847989036"}},"outputId":"76c1da7e-cca9-42a6-f879-66646b3a2080"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Montar Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import numpy as np\n","from scipy import optimize\n","import pandas as pd"],"metadata":{"id":"EXvSqaM5zeCU","executionInfo":{"status":"ok","timestamp":1726741909424,"user_tz":240,"elapsed":418,"user":{"displayName":"Charly Alexandre Terrazas Rosado","userId":"18005393683847989036"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Cargar el dataset\n","data_path = '/content/drive/MyDrive/Colab Notebooks/IA/Primer Parcial/Cleaned_data_for_model.csv'\n","data = pd.read_csv(data_path)\n","\n","# Normalizar datos numéricos para el entrenamiento.\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","data[['price', 'baths', 'bedrooms', 'Area_in_Marla']] = scaler.fit_transform(data[['price', 'baths', 'bedrooms', 'Area_in_Marla']])\n","\n","# Convertir columnas categóricas en variables dummy (variables indicadoras).\n","data = pd.get_dummies(data, columns=['property_type', 'location', 'city', 'purpose'], drop_first=True)\n","\n","# Convertir a un array de NumPy.\n","data_array = data.values"],"metadata":{"id":"VZ0uTl-LzhrR","executionInfo":{"status":"ok","timestamp":1726741946598,"user_tz":240,"elapsed":10510,"user":{"displayName":"Charly Alexandre Terrazas Rosado","userId":"18005393683847989036"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Separar características (X) y etiquetas (y)\n","X = data_array[:, :-1]  # Todas las columnas menos la última.\n","y = data_array[:, -1]   # Última columna como etiqueta.\n","\n","# Dividir el dataset en entrenamiento (80%) y prueba (20%).\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Añadir la columna de unos para el término bias.\n","X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n","X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n","\n","# Verificar las dimensiones.\n","print(\"Dimensiones de X_train:\", X_train.shape)\n","print(\"Dimensiones de y_train:\", y_train.shape)\n","print(\"Dimensiones de X_test:\", X_test.shape)\n","print(\"Dimensiones de y_test:\", y_test.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCpukL9q12Km","executionInfo":{"status":"ok","timestamp":1726742033902,"user_tz":240,"elapsed":65574,"user":{"displayName":"Charly Alexandre Terrazas Rosado","userId":"18005393683847989036"}},"outputId":"1981a50d-cd3c-4974-a338-686b9baa3a9c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimensiones de X_train: (79599, 1404)\n","Dimensiones de y_train: (79599,)\n","Dimensiones de X_test: (19900, 1404)\n","Dimensiones de y_test: (19900,)\n"]}]},{"cell_type":"code","source":["# Función sigmoide.\n","def sigmoid(z):\n","    \"\"\"Función sigmoide.\"\"\"\n","    return 1 / (1 + np.exp(-z))\n","\n","# Función de coste con regularización.\n","def costFunction(theta, X, y, lambda_):\n","    \"\"\"Función de coste para la regresión logística.\"\"\"\n","    theta = np.array(theta, dtype=np.float64)\n","    X = np.array(X, dtype=np.float64)\n","    y = np.array(y, dtype=np.float64)\n","\n","    m = y.size\n","    h = sigmoid(X.dot(theta))\n","    cost = (-1 / m) * (y.dot(np.log(h)) + (1 - y).dot(np.log(1 - h)))\n","    reg = (lambda_ / (2 * m)) * np.sum(np.square(theta[1:]))  # Regularización\n","    return cost + reg\n","\n","# Función de gradiente con regularización.\n","def gradient(theta, X, y, lambda_):\n","    \"\"\"Función de gradiente.\"\"\"\n","    theta = np.array(theta, dtype=np.float64)\n","    X = np.array(X, dtype=np.float64)\n","    y = np.array(y, dtype=np.float64)\n","\n","    m = y.size\n","    h = sigmoid(X.dot(theta))\n","    grad = (1 / m) * X.T.dot(h - y)\n","    reg = np.concatenate([[0], (lambda_ / m) * theta[1:]])  # Regularización para no sumar el bias.\n","    return grad + reg\n"],"metadata":{"id":"7lFDS3mm15FA","executionInfo":{"status":"ok","timestamp":1726742036951,"user_tz":240,"elapsed":626,"user":{"displayName":"Charly Alexandre Terrazas Rosado","userId":"18005393683847989036"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Definir la función para entrenar múltiples modelos de regresión logística (One-vs-All).\n","def oneVsAll(X, y, num_labels, lambda_):\n","    \"\"\"Entrenar múltiples modelos de regresión logística (One-vs-All).\"\"\"\n","    X = np.array(X, dtype=np.float64)\n","    y = np.array(y, dtype=np.float64)\n","\n","    m, n = X.shape\n","    all_theta = np.zeros((num_labels, n), dtype=np.float64)\n","    for i in range(num_labels):\n","        initial_theta = np.zeros(n, dtype=np.float64)\n","        # Entrenar usando optimización (minimización de coste).\n","        theta = optimize.fmin_tnc(func=costFunction, x0=initial_theta, fprime=gradient, args=(X, (y == i).astype(int), lambda_))[0]\n","        all_theta[i] = theta\n","    return all_theta\n","\n","# Número de etiquetas (suponiendo que las etiquetas son enteros consecutivos).\n","num_labels = len(np.unique(y_train))  # Ajustar según el dataset.\n","\n","# Parámetro de regularización.\n","lambda_ = 0.1\n","\n","# Entrenar el modelo y obtener los parámetros para cada clase.\n","all_theta = oneVsAll(X_train, y_train, num_labels, lambda_)\n","print(\"Parámetros entrenados para cada clase (all_theta):\\n\", all_theta)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYhqLkzy2KRi","executionInfo":{"status":"ok","timestamp":1726742288760,"user_tz":240,"elapsed":238923,"user":{"displayName":"Charly Alexandre Terrazas Rosado","userId":"18005393683847989036"}},"outputId":"3998a368-0dba-444c-9f12-981b7f2d4ca8"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-21-217f1f3d45e8>:15: RuntimeWarning: divide by zero encountered in log\n","  cost = (-1 / m) * (y.dot(np.log(h)) + (1 - y).dot(np.log(1 - h)))\n"]},{"output_type":"stream","name":"stdout","text":["Parámetros entrenados para cada clase (all_theta):\n"," [[-5.58267565e-06 -7.54182396e-06 -2.14390630e-05 ... -2.58229619e-06\n","  -7.27632528e-06  2.42951566e-07]\n"," [ 3.26541070e+00 -3.93199566e-06  9.22940474e+00 ...  8.21687513e-01\n","   2.23893359e+00  1.22332122e-02]]\n"]}]},{"cell_type":"code","source":["# Función de predicción utilizando los parámetros entrenados (One-vs-All).\n","def predictOneVsAll(all_theta, X):\n","    \"\"\"Predecir utilizando los parámetros entrenados.\"\"\"\n","    X = np.array(X, dtype=np.float64)\n","    return np.argmax(sigmoid(X.dot(all_theta.T)), axis=1)\n","\n","# Calcular predicciones en el conjunto de prueba.\n","predictions = predictOneVsAll(all_theta, X_test)\n","\n","# Calcular la precisión del modelo.\n","accuracy = np.mean(predictions == y_test) * 100\n","print(f'Precisión del modelo en el conjunto de prueba: {accuracy:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SpNZUxu62M1U","executionInfo":{"status":"ok","timestamp":1726742290277,"user_tz":240,"elapsed":24,"user":{"displayName":"Charly Alexandre Terrazas Rosado","userId":"18005393683847989036"}},"outputId":"0f0c0243-b36b-4c8a-e3c1-54f4d6b64d71"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Precisión del modelo en el conjunto de prueba: 99.42%\n"]}]}]}